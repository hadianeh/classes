<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=windows-1252">
    <link rel="stylesheet" type="text/css" href="style.css">
    <title>CSE 240D</title>
  </head>
  <body>
    <div class="container">
      <!-- Header -->
      <div id="intro">
        <h2> CSE 240D: System Design for Deep Learning (Fall 2024)</h2>
        <br>
        <h3>Instructor: <a href="http://cseweb.ucsd.edu/%7Ehadi">Hadi Esmaeilzadeh</a></h3>
        <br>
        <h5>Email: hadi [AT] ucsd [DOT] edu</h5>
        <h5>Office: CSE 3228 </h5>
        <h5>Office hours: by appointment</h5>
        <h5><br></h5>
        <h5>TA:</h5>
        <h5>
          <a href="https://christopherpriebe.github.io/">Christopher Priebe</a> cpriebe [AT] ucsd [DOT] edu
        </h5>
        <h5>Office hours: by appointment in CSE 3254</h5>
      </div>
      <div id="navbar">
        <ul class="menu">
          <li><a href="index.html">Syllabus</a></li>
          <li><a href="schedule.html">Schedule</a></li>
          <li><a href="presentation.html">Presentation</a></li>
          <li><a href="critique.html">Critique</a></li>
          <li><a href="project.html">Project</a></li>
        </ul>
      </div>
      <!-- Main Content -->
      <div id="leftcolumn">
        <p>
          This is the anticipated schedule for the course.
          The schedule is subject to change.
          Each reading is annotated with a tag encased in square brackets.
          The tags are as follows:
          <ul>
            <li>[Read]: Required reading</li>
            <li>[Optional]: Optional reading</li>
            <li>[C&C]: Write and submit a compare and contrast (C&C) for the two papers</li>
            <li>[Critique]: Write and submit a criqitue for the paper; if there are two papers that week, PICK AND CRITIQUE ONE</li>
            <li>[SP]: Student will present this work in class</li>
          </ul>
        For more information on critiques and C&Cs, please refer to the <a href="critique.html">critique</a> page.
        </p>
        <table class="tg">
          <tr>
            <th class="tg-14gg"></th>
            <th class="tg-14gg">Lecture</th>
            <th class="tg-14gg">Date</th>
            <th class="tg-14gg">Presenters</th>
          </tr>
          <tr>
            <td class="tg-jkmc">1, 2</td>
            <td class="tg-jkmc">Dark Silicon and the Need for Acceleration (<a href="https://drive.google.com/file/d/1t9VnqEKk76EDfodf6Z405MCWI8hmebK5/view?usp=sharing">slides</a>)</br>
            <ul>
              <li>[Read] <a href="https://dl.acm.org/doi/10.1145/2000064.2000108">Dark Silicon and the End of Multicore Scaling</a></li>
              <li>[Read] <a href="https://dl.acm.org/doi/10.1145/1465482.1465560">Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities</a></li>
              <li>[Optional] <a href="https://bpb-us-w2.wpmucdn.com/sites.coecis.cornell.edu/dist/7/587/files/2023/06/ESMAEILZADEH_2011_DARK.pdf">RETROSPECTIVE: Dark Silicon and the End of Multicore Scaling</a></li>
              <li>[Optional] <a href="https://www.nytimes.com/2011/08/01/science/01chips.html">Progress Hits Snag: Tiny Chips Use Outsize Power</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1145/2408776.2408797">Power Challenges May End the Multicore Era</a></li>
            </ul>
            </td>
            <td class="tg-buh4">09/30/2024</br>10/02/2024</td>
            <td class="tg-buh4">Hadi</td>
          </tr>
          <tr>
            <td class="tg-jkmc">3, 4</td>
            <td class="tg-jkmc">Towards Hardware Acceleration of ML (<a href="https://drive.google.com/file/d/1rPdjuG-yZbt68ZOFbXq3FAKv9au1pOec/view?usp=sharing">slides</a>)</br>
            <ul>
              <li>[Read] <a href="https://ieeexplore.ieee.org/abstract/document/7446050">TABLA: A Unified Template-Based Framework for Accelerating Statistical Machine Learning</a></li>
              <li>[Read] <a href="https://dl.acm.org/doi/10.5555/3195638.3195659">From High-Level Deep Neural Models to FPGAs</a></li>
            </ul>
            <td class="tg-buh4">10/07/2024</br>10/09/2024</td>
            <td class="tg-buh4">Hadi</td>
          </tr>
          <tr>
            <td class="tg-jkmc">5, 6</td>
            <td class="tg-jkmc">Historical Perspective on Neural Networks</br>
            <ul>
              <li>[Optional] <a href="https://d2l.ai/">Dive Into Deep Learning</a> Chapters <a href="https://d2l.ai/chapter_introduction/index.html">1</a>, <a href="https://d2l.ai/chapter_preliminaries/index.html">2</a>, <a href="https://d2l.ai/chapter_multilayer-perceptrons/index.html">5</a>, <a href="https://d2l.ai/chapter_optimization/index.html">12</a></li>
              <li>[Optional] <a href="https://www.nature.com/articles/323533a0">Learning Representations by Back-Propagating Errors</a></li>
              <li>[Optional] <a href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></li>
              <li>[Optional] <a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full">A Stochastic Approximation Method</a></li>
            </ul>
            </td>
            <td class="tg-buh4">10/14/2024</br>10/16/2024</td>
            <td class="tg-buh4">Hadi</td>
          </tr>
          <tr>
            <td class="tg-jkmc">7</td>
            <td class="tg-jkmc">Introduction to DNN Accelerators</br>
            <ul>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1109/ISCA.2016.40">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</a></li>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1145/3352460.3358302">Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture</a></li>
              <li>[Optional] <a href="https://ieeexplore.ieee.org/document/1653825">Why Systolic Architectures?</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1145/1356052.1356053">Anatomy of High-Performance Matrix Multiplication</a></li>
              <li>[Optional] <a href="https://sites.coecis.cornell.edu/isca50retrospective/files/2023/06/chen_2016_eyeriss.pdf">RETROSPECTIVE: Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1109/MICRO.2014.58">DaDianNao: A Machine-Learning Supercomputer</a></li>
            </ul>
            <td class="tg-buh4">10/21/2024</td>
            <td class="tg-buh4">Samhita Varambally</br>Suraj Sathya-Prakash</br></br>Ozgur Ozerdem</br>Divij Divij</td>
          </tr>
          <tr>
            <td class="tg-jkmc">8</td>
            <td class="tg-jkmc">Sparse Acceleration of DNNs</br>
            <ul>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1145/3007787.3001163">EIE: Efficient Inference Engine on Compressed Deep Neural Network</a></li>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/abs/10.1145/3140659.3080254">SCNN: An Accelerator for Compressed-Sparse Convolutional Neural Networks</a></li>
              <li>[Optional] <a href="https://sites.coecis.cornell.edu/isca50retrospective/files/2023/06/Han_2016_EIE.pdf">Retrospective: EIE: Efficient Inference Engine on Sparse and Compressed Neural Network</a></li>
              <li>[Optional] <a href="https://ieeexplore.ieee.org/document/9507542">Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights</a></li>
            </ul>
            <td class="tg-buh4">10/23/2024</td>
            <td class="tg-buh4">Jheng-Ying Lin</br>Ya-Chi Liao</br></br>Ashwin Rohit Alagiri Rajan</br>Rishabh Kumar</td>
          </tr>
          <tr>
            <td class="tg-jkmc">9</td>
            <td class="tg-jkmc">Bit-Flexible Acceleration of DNNs</br>
            <ul>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1109/ISCA.2018.00069">Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks</a></li>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.5555/3195638.3195661">Stripes: Bit-Serial Deep Neural Network Computing</a></li>
              <li>[Optional] <a href="https://sites.coecis.cornell.edu/isca50retrospective/files/2023/06/Sharma_2018_Bit.pdf">RETROSPECTIVE: Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1145/3123939.3123982">Bit-Pragmatic Deep Neural Network Computing</a></li>
              <li>[Optional] <a href="https://ieeexplore.ieee.org/document/8481682">UNPU: An Energy-Efficient Deep Neural Network Accelerator With Fully Variable Weight Bit Precision</a></li>
            </ul>
            <td class="tg-buh4">10/28/2024</td>
            <td class="tg-buh4">Anish Govind</br></br>Sin-Yu Chen</td>
          </tr>
          <tr>
            <td class="tg-jkmc">10</td>
            <td class="tg-jkmc">Multi-Tenant Acceleration of DNNs</br>
            <ul>
              <li>[C&C] [SP] <a href="https://ieeexplore.ieee.org/document/9251939">Planaria: Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks</a></li>
              <li>[C&C] [SP] <a href="https://ieeexplore.ieee.org/abstract/document/9065590">PREMA: A Predictive Multi-Task Scheduling Algorithm For Preemptible Neural Processing Units</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1145/3079856.3080221">Maximizing CNN Accelerator Efficiency Through Resource Partitioning</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1109/ISCA45697.2020.00081">A Multi-Neural Network Acceleration Architecture</a></li>
            </ul>
            <td class="tg-buh4">10/30/2024</td>
            <td class="tg-buh4">Yongshi Zhan</br>Pranith Kallakuri</br></br>Garrett Hilyer</td>
          </tr>
          <!-- <tr>
            <td class="tg-jkmc"></td>
            <td class="tg-jkmc">Mixed-Signal Acceleration of DNNs</br>
            <ul>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/abs/10.1145/3410463.3414634">Mixed-Signal Charge-Domain Acceleration of Deep Neural Networks through Interleaved Bit-Partitioned Arithmetic</a></li>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1145/3007787.3001139">ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1109/ISCA.2018.00015">PROMISE: An End-to-End Design of a Programmable Mixed-Signal Accelerator for Machine-Learning Algorithms</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1145/3007787.3001164">RedEye: Analog ConvNet Image Sensor Architecture for Continuous Mobile Vision</a></li>
            </ul>
            <td class="tg-buh4"></td>
            <td class="tg-buh4">TBD</br></br>TBD</td>
          </tr> -->
          <tr>
            <td class="tg-jkmc">12</td>
            <td class="tg-jkmc">Acceleration of Emerging DNNs</br>
            <ul>
              <li>[Critique] [SP] <a href="https://dl.acm.org/doi/10.1145/3620665.3640365">Tandem Processor: Grappling with Emerging Operators in Neural Networks</a></li>
              <li><a href="https://actlab-genesys.github.io/">GeneSys: Open-Source Parameterizable NPU Generator with Full-Stack Multi-Target Compilation Stack</a></li>
            </ul>
            <td class="tg-buh4">11/04/2024</td>
            <td class="tg-buh4">Yuchuan Li</br></br>Hanyang Xu</td>
          </tr>
          <tr>
            <td class="tg-jkmc">11</td>
            <td class="tg-jkmc">TPU</br>
            <ul>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1145/3079856.3080246">In-Datacenter Performance Analysis of a Tensor Processing Unit</a></li>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/abs/10.1145/3579371.3589350">TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings</a></li>
              <li>[Optional] <a href="https://sites.coecis.cornell.edu/isca50retrospective/files/2023/06/Jouppi_2017_In_Datacenter.pdf">Retrospective on "In-Datacenter Performance Analysis of a Tensor Processing Unit"</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1109/ISCA52012.2021.00010">Ten Lessons from Three Generations Shaped Google's TPUv4i</a></li>
            </ul>
            <td class="tg-buh4">11/06/2024</td>
            <td class="tg-buh4">Mingrui Yin</br></br>Luting Lei</td>
          </tr>
          <tr>
            <td class="tg-jkmc">13</td>
            <td class="tg-jkmc">Compilation and Optimization of DNNs</br>
            <ul>
              <li>[C&C] [SP] <a href="https://www.usenix.org/conference/osdi18/presentation/chen">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></li>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1145/3133901">The Tensor Algebra Compiler</a></li>
              <li>[Optional] <a href="https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html">A Friendly Introduction to Machine Learning Compilers and Optimizers</a></li>
              <li>[Optional] <a href="https://arxiv.org/abs/2002.03794">The Deep Learning Compiler: A Comprehensive Survey</a></li>
            </ul>
            <td class="tg-buh4">11/13/2024</td>
            <td class="tg-buh4">Wentao Ni</br>Peiyuan Zhang</br></br>Devanshi Panchal</td>
          </tr>
          <tr>
            <td class="tg-jkmc">14</td>
            <td class="tg-jkmc">Compilation of Cross-Domain Applications for Accelerators</br>
            <ul>
              <li>[C&C] [SP] <a href="https://ieeexplore.ieee.org/document/9407145">A Computational Stack for Cross-Domain Acceleration</a></li>
              <li>[C&C] [SP] <a href="https://dl.acm.org/doi/10.1109/CGO51591.2021.9370308">MLIR: Scaling Compiler Infrastructure for Domain Specific Computation</a></li>
              <li>[Optional] <a href="https://dl.acm.org/doi/10.1109/MM.2022.3189416">Yin-Yang: Programming Abstractions for Cross-Domain Multi-Acceleration</a></li>
            </ul>
            <td class="tg-buh4">11/18/2024</td>
            <td class="tg-buh4">Alvin Cheng</br></br>Prashil Parekh</td>
          </tr>
          <tr>
            <td class="tg-jkmc">15</td>
            <td class="tg-jkmc">Introduction to Transformers</br>
            <ul>
              <li>[Critique] [SP] <a href="https://dl.acm.org/doi/10.5555/3295222.3295349">Attention is All You Need</a></li>
              <li>[Optional] <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
              <li>[Optional] <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
              <li>[Optional] <a href="https://d2l.ai/">Dive Into Deep Learning</a> Chapter <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html">11</a></li>
            </ul>
            <td class="tg-buh4">11/20/2024</td>
            <td class="tg-buh4">Hadi</br>TBD</td>
          </tr>
          <tr>
            <td class="tg-jkmc">16</td>
            <td class="tg-jkmc">Systems for DNNs</br>
            <ul>
              <li>[Critique; CHOOSE ONE] [SP] <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
              <li>[Critique; CHOOSE ONE] [SP] <a href="https://dl.acm.org/doi/10.1145/3620666.3651335">SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification</a></li>
            </ul>
            <td class="tg-buh4">11/25/2024</td>
            <td class="tg-buh4">TBD</br></br>Xuan Wang</td>
          </tr>
          <tr>
            <td class="tg-jkmc">17</td>
            <td class="tg-jkmc">End-to-End Applications with ML</br>
            <ul>
              <li>[Critique; CHOOSE ONE] [SP] <a href="https://dl.acm.org/doi/10.1145/3173162.3173191">The Architectural Implications of Autonomous Driving: Constraints and Acceleration</a></li>
              <li>[Critique; CHOOSE ONE] [SP] <a href="https://dl.acm.org/doi/abs/10.5555/3495724.3496517">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
            </ul>
            <td class="tg-buh4">12/02/2024</td>
            <td class="tg-buh4">Shrideep Koparkar</br></br>Ashwin Ramachandran</td>
          </tr>
          <tr>
            <td class="tg-jkmc">18</td>
            <td class="tg-jkmc">Emerging Applications in ML</br>
            <ul>
              <li>[Critique; CHOOSE ONE] [SP] <a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a></li>
              <li>[Critique; CHOOSE ONE] [SP] <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></li>
            </ul>
            <td class="tg-buh4">12/04/2024</td>
            <td class="tg-buh4">TBD</br></br>Chia-Chi Sung</td>
          </tr>
        </table>
      </div>
      <!-- Footer -->
      <div id="footer"> Last update: November 2024 </div>
    </div>
  </body>
</html>
